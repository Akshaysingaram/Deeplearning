{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check\n",
      "tensor([2., 2., 2., 2., 2.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device=torch.device(\"cuda\")\n",
    "    print(\"check\")\n",
    "    x=torch.ones(5, device=device)\n",
    "    y=torch.ones(5)\n",
    "    y=y.to(device)\n",
    "    z=x+y\n",
    "    #z=z.to(\"cpu\")\n",
    "    #z.numpy()\n",
    "    print(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x=torch.tensor(1.0)\n",
    "y=torch.tensor(2.0)\n",
    "\n",
    "w=torch.tensor(3.0,requires_grad=True)\n",
    "\n",
    "y_hat=w*x\n",
    "loss=(y_hat-y)**2\n",
    "print(loss)\n",
    "\n",
    "#backward pass\n",
    "loss.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training: f(5)= 2000.000\n",
      "epoch 1:w =161.200, loss= 1188030.00000000\n",
      "epoch 2:w =65.680, loss= 190084.78125000\n",
      "epoch 3:w =27.472, loss= 30413.56835938\n",
      "epoch 4:w =12.189, loss= 4866.17089844\n",
      "epoch 5:w =6.076, loss= 778.58715820\n",
      "epoch 6:w =3.630, loss= 124.57394409\n",
      "epoch 7:w =2.652, loss= 19.93183136\n",
      "epoch 8:w =2.261, loss= 3.18909311\n",
      "epoch 9:w =2.104, loss= 0.51025498\n",
      "epoch 10:w =2.042, loss= 0.08164060\n",
      "epoch 11:w =2.017, loss= 0.01306249\n",
      "epoch 12:w =2.007, loss= 0.00209002\n",
      "epoch 13:w =2.003, loss= 0.00033440\n",
      "epoch 14:w =2.001, loss= 0.00005351\n",
      "epoch 15:w =2.000, loss= 0.00000856\n",
      "epoch 16:w =2.000, loss= 0.00000137\n",
      "epoch 17:w =2.000, loss= 0.00000022\n",
      "epoch 18:w =2.000, loss= 0.00000004\n",
      "epoch 19:w =2.000, loss= 0.00000001\n",
      "epoch 20:w =2.000, loss= 0.00000000\n",
      "Prediction after training :f(5) = 14.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#f=w*x\n",
    "#f=2*x\n",
    "\n",
    "X=np.array([1,2,3,4], dtype=np.float32)\n",
    "Y=np.array([2,4,6,8], dtype=np.float32)\n",
    "\n",
    "w=0.0\n",
    "#model prediction\n",
    "def forward(x):\n",
    "    return w*x;\n",
    "\n",
    "#loss\n",
    "def loss(y,y_predicted):\n",
    "    return ((y_predicted-y)**2).mean()\n",
    "\n",
    "#gradient\n",
    "# MSE = 1/N * (w*x - y)**2\n",
    "# dJ/dw= 1/N * 2x * (w*x - y)\n",
    "\n",
    "def gradient(x,y,y_predicted):\n",
    "    return np.dot(2*x,y_predicted-y).mean()\n",
    "\n",
    "print(f'prediction before training: f(5)= {forward(5):.3f}')\n",
    "\n",
    "#train\n",
    "learning_rate= 0.01\n",
    "n_iters=20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #prediction = forward pass\n",
    "    y_pred= forward(X)\n",
    "\n",
    "    #loss\n",
    "    l=loss(Y,y_pred)\n",
    "    \n",
    "    #gradients\n",
    "    dw= gradient(X,Y,y_pred)\n",
    "\n",
    "    #update weights\n",
    "    w-=learning_rate*dw\n",
    "    \n",
    "    if epoch%1==0:\n",
    "        print(f'epoch {epoch+1}:w ={w:.3f}, loss= {l:.8f}')\n",
    "print(f'Prediction after training :f(5) = {forward(7):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training: f(5)= 0.000\n",
      "epoch 1:w =0.300, loss= 30.00000000\n",
      "epoch 11:w =1.665, loss= 1.16278565\n",
      "epoch 21:w =1.934, loss= 0.04506890\n",
      "epoch 31:w =1.987, loss= 0.00174685\n",
      "epoch 41:w =1.997, loss= 0.00006770\n",
      "epoch 51:w =1.999, loss= 0.00000262\n",
      "epoch 61:w =2.000, loss= 0.00000010\n",
      "epoch 71:w =2.000, loss= 0.00000000\n",
      "epoch 81:w =2.000, loss= 0.00000000\n",
      "epoch 91:w =2.000, loss= 0.00000000\n",
      "Prediction after training :f(5) = 14.000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#f=w*x\n",
    "#f=2*x\n",
    "\n",
    "X=torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "Y=torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "\n",
    "w=torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "#model prediction\n",
    "def forward(x):\n",
    "    return w*x;\n",
    "\n",
    "#loss\n",
    "def loss(y,y_predicted):\n",
    "    return ((y_predicted-y)**2).mean()\n",
    "\n",
    "#gradient\n",
    "# MSE = 1/N * (w*x - y)**2\n",
    "# dJ/dw= 1/N * 2x * (w*x - y)\n",
    "\n",
    "print(f'prediction before training: f(5)= {forward(5):.3f}')\n",
    "\n",
    "#train\n",
    "learning_rate= 0.01\n",
    "n_iters=100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #prediction = forward pass\n",
    "    y_pred= forward(X)\n",
    "\n",
    "    #loss\n",
    "    l=loss(Y,y_pred)\n",
    "    \n",
    "    #gradients=backward pass\n",
    "    l.backward()\n",
    "\n",
    "    #update weights\n",
    "    with torch.no_grad():\n",
    "        w-=learning_rate*w.grad\n",
    "    \n",
    "    #zero gradients\n",
    "    w.grad.zero_()\n",
    "\n",
    "    if epoch%10==0:\n",
    "        print(f'epoch {epoch+1}:w ={w:.3f}, loss= {l:.8f}')\n",
    "print(f'Prediction after training :f(5) = {forward(7):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "prediction before training: f(5)= -3.414\n",
      "epoch 1:w =-0.377, loss= 56.78449249\n",
      "epoch 11:w =1.357, loss= 2.07409954\n",
      "epoch 21:w =1.652, loss= 0.62765229\n",
      "epoch 31:w =1.714, loss= 0.56107229\n",
      "epoch 41:w =1.738, loss= 0.53189045\n",
      "epoch 51:w =1.755, loss= 0.50527346\n",
      "epoch 61:w =1.772, loss= 0.48022902\n",
      "epoch 71:w =1.787, loss= 0.45664245\n",
      "epoch 81:w =1.803, loss= 0.43442911\n",
      "epoch 91:w =1.817, loss= 0.41350842\n",
      "Prediction after training :f(5) = 10.032\n",
      "time taken is -> 0.031\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#1. Design model (input, output size, forward pass)\n",
    "#2. Construct loss and optimizer\n",
    "#3. Training loop\n",
    "\n",
    "#Pytorch pipeline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import time \n",
    "\n",
    "X=torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
    "Y=torch.tensor([[2],[4],[6],[9]], dtype=torch.float32)\n",
    "\n",
    "X_test=torch.tensor([5], dtype=torch.float32)\n",
    "n_samples, n_features=X.shape\n",
    "print(n_samples,n_features)\n",
    "\n",
    "input_size=n_features\n",
    "output_size=n_features\n",
    "\n",
    "#model= nn.Linear(input_size,output_size)\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "\n",
    "    def __init__(self,input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        #define layers\n",
    "        self.lin= nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "model= LinearRegression(input_size, output_size)\n",
    "\n",
    "\n",
    "print(f'prediction before training: f(5)= {model(X_test).item():.3f}')\n",
    "\n",
    "#train\n",
    "learning_rate= 0.01\n",
    "n_iters=100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "start= time.time()\n",
    "for epoch in range(n_iters):\n",
    "    #prediction = forward pass\n",
    "    y_pred= model(X)\n",
    "\n",
    "    #loss\n",
    "    l=loss(Y,y_pred)\n",
    "    \n",
    "    #gradients=backward pass\n",
    "    l.backward()\n",
    "\n",
    "    #update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    #zero gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch%10==0:\n",
    "        [w, b]= model.parameters()\n",
    "        print(f'epoch {epoch+1}:w ={w[0][0].item():.3f}, loss= {l:.8f}')\n",
    "print(f'Prediction after training :f(5) = {model(X_test).item():.3f}')\n",
    "end=time.time()\n",
    "print(f'time taken is -> {(end-start):.3f}')\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, loss= 4397.6357\n",
      "epoch 20, loss= 3282.9006\n",
      "epoch 30, loss= 2475.6409\n",
      "epoch 40, loss= 1890.4598\n",
      "epoch 50, loss= 1465.8682\n",
      "epoch 60, loss= 1157.5322\n",
      "epoch 70, loss= 933.4430\n",
      "epoch 80, loss= 770.4635\n",
      "epoch 90, loss= 651.8497\n",
      "epoch 100, loss= 565.4713\n",
      "epoch 110, loss= 502.5327\n",
      "epoch 120, loss= 456.6494\n",
      "epoch 130, loss= 423.1843\n",
      "epoch 140, loss= 398.7659\n",
      "epoch 150, loss= 380.9415\n",
      "epoch 160, loss= 367.9256\n",
      "epoch 170, loss= 358.4181\n",
      "epoch 180, loss= 351.4712\n",
      "epoch 190, loss= 346.3940\n",
      "epoch 200, loss= 342.6821\n",
      "epoch 210, loss= 339.9679\n",
      "epoch 220, loss= 337.9829\n",
      "epoch 230, loss= 336.5309\n",
      "epoch 240, loss= 335.4685\n",
      "epoch 250, loss= 334.6911\n",
      "epoch 260, loss= 334.1222\n",
      "epoch 270, loss= 333.7058\n",
      "epoch 280, loss= 333.4011\n",
      "epoch 290, loss= 333.1779\n",
      "epoch 300, loss= 333.0146\n",
      "epoch 310, loss= 332.8950\n",
      "epoch 320, loss= 332.8073\n",
      "epoch 330, loss= 332.7432\n",
      "epoch 340, loss= 332.6962\n",
      "epoch 350, loss= 332.6618\n",
      "epoch 360, loss= 332.6366\n",
      "epoch 370, loss= 332.6181\n",
      "epoch 380, loss= 332.6046\n",
      "epoch 390, loss= 332.5947\n",
      "epoch 400, loss= 332.5875\n",
      "epoch 410, loss= 332.5822\n",
      "epoch 420, loss= 332.5782\n",
      "epoch 430, loss= 332.5754\n",
      "epoch 440, loss= 332.5733\n",
      "epoch 450, loss= 332.5717\n",
      "epoch 460, loss= 332.5707\n",
      "epoch 470, loss= 332.5699\n",
      "epoch 480, loss= 332.5692\n",
      "epoch 490, loss= 332.5688\n",
      "epoch 500, loss= 332.5685\n",
      "epoch 510, loss= 332.5682\n",
      "epoch 520, loss= 332.5681\n",
      "epoch 530, loss= 332.5679\n",
      "epoch 540, loss= 332.5678\n",
      "epoch 550, loss= 332.5677\n",
      "epoch 560, loss= 332.5677\n",
      "epoch 570, loss= 332.5676\n",
      "epoch 580, loss= 332.5677\n",
      "epoch 590, loss= 332.5676\n",
      "epoch 600, loss= 332.5676\n",
      "epoch 610, loss= 332.5676\n",
      "epoch 620, loss= 332.5676\n",
      "epoch 630, loss= 332.5676\n",
      "epoch 640, loss= 332.5676\n",
      "epoch 650, loss= 332.5676\n",
      "epoch 660, loss= 332.5676\n",
      "epoch 670, loss= 332.5676\n",
      "epoch 680, loss= 332.5676\n",
      "epoch 690, loss= 332.5675\n",
      "epoch 700, loss= 332.5676\n",
      "epoch 710, loss= 332.5675\n",
      "epoch 720, loss= 332.5676\n",
      "epoch 730, loss= 332.5675\n",
      "epoch 740, loss= 332.5676\n",
      "epoch 750, loss= 332.5676\n",
      "epoch 760, loss= 332.5676\n",
      "epoch 770, loss= 332.5676\n",
      "epoch 780, loss= 332.5675\n",
      "epoch 790, loss= 332.5676\n",
      "epoch 800, loss= 332.5676\n",
      "epoch 810, loss= 332.5676\n",
      "epoch 820, loss= 332.5676\n",
      "epoch 830, loss= 332.5676\n",
      "epoch 840, loss= 332.5676\n",
      "epoch 850, loss= 332.5676\n",
      "epoch 860, loss= 332.5676\n",
      "epoch 870, loss= 332.5676\n",
      "epoch 880, loss= 332.5676\n",
      "epoch 890, loss= 332.5676\n",
      "epoch 900, loss= 332.5676\n",
      "epoch 910, loss= 332.5676\n",
      "epoch 920, loss= 332.5676\n",
      "epoch 930, loss= 332.5676\n",
      "epoch 940, loss= 332.5676\n",
      "epoch 950, loss= 332.5676\n",
      "epoch 960, loss= 332.5676\n",
      "epoch 970, loss= 332.5676\n",
      "epoch 980, loss= 332.5676\n",
      "epoch 990, loss= 332.5676\n",
      "epoch 1000, loss= 332.5676\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#0. prep data\n",
    "\n",
    "X_numpy, y_numpy = datasets.make_regression(n_samples=100,n_features=1,noise=20,random_state=1)\n",
    "X= torch.from_numpy(X_numpy.astype(np.float32))\n",
    "y= torch.from_numpy(y_numpy.astype(np.float32))\n",
    "y=y.view(y.shape[0], 1)\n",
    "\n",
    "n_samples, n_features= X.shape\n",
    "\n",
    "#1. model\n",
    "input_size=n_features\n",
    "output_size=1\n",
    "model= nn.Linear(input_size, output_size)\n",
    "\n",
    "#2. loss\n",
    "criterion= nn.MSELoss()\n",
    "learning_rate=0.01\n",
    "optimizer= torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "#3. training loop\n",
    "num_epochs=1000\n",
    "for epoch in range(num_epochs):\n",
    "    #forward pass and loss\n",
    "    y_predicted= model(X)\n",
    "    loss = criterion(y_predicted,y)\n",
    "\n",
    "    #backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    #update\n",
    "    optimizer.step()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch+1)%10 == 0:\n",
    "        print(f'epoch {epoch+1}, loss= {loss.item():.4f}')\n",
    "#plot\n",
    "predicted= model(X).detach().numpy()\n",
    "plt.plot(X_numpy,y_numpy,'ro')\n",
    "plt.plot(X_numpy,predicted, 'b')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61d25cd076709389340f134b58b57275c3581b50014a1c57c07a1adc674108e2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('Deeplearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
